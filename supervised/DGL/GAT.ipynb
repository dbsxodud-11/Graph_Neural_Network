{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitfbc3adf34ccb4b3ca0f454a24514bd21",
   "display_name": "Python 3.7.7 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from dgl.data import citation_graph\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DGLGraph(num_nodes=2708, num_edges=10556,\n         ndata_schemes={}\n         edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# Load Cora Dataset\n",
    "def load_cora_data() :\n",
    "    data = citation_graph.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    mask = torch.BoolTensor(data.train_mask)\n",
    "    g = dgl.DGLGraph(data.graph)\n",
    "\n",
    "    return g, features, labels, mask\n",
    "g, features, labels, mask = load_cora_data()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAT(\n  (layers): ModuleList(\n    (0): Linear(in_features=1433, out_features=8, bias=True)\n    (1): Linear(in_features=8, out_features=7, bias=True)\n  )\n  (activations): ModuleList(\n    (0): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# two-layer GAT model\n",
    "class GATLayer(nn.Module) :\n",
    "    def __init__(self, g, input_dim, output_dim) :\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(in_dim, out_dim, bias=False))\n",
    "        self.layers.append(nn.Linear(out_dim*2, 1, bias=False)) # Attention\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) :\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        for layer in self.layers :\n",
    "            nn.init.xavier_normal_(layer.weight, gain=gain)\n",
    "\n",
    "    def forward(self, h) :\n",
    "        \n",
    "        z = self.layers[0](h) # Equation 1\n",
    "        self.g.ndata['h'] = z\n",
    "        self.g.apply_edges(func=self.edge_attention) # Equation 2\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')\n",
    "\n",
    "    def edge_attention(self, edges) :\n",
    "        z_ij = torch.cat([edges.src['z'], edges.dst['z']], dim=-1)\n",
    "        e_ij = F.leaky_relu(self.layers[1](z_ij))\n",
    "        return {'e' : e_ij}\n",
    "\n",
    "    def message_func(self, edges) :\n",
    "        return {'z' : edges.src['z'], 'e' : edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes) :\n",
    "        a_ij = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        h = torch.sum(a_ij * node.mailbox['z'], dim=1)\n",
    "        return {'h' : h}\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module) :\n",
    "    def __init__(self, g, input_dim, output_dim, num_heads, merge=\"cat\") :\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads) :\n",
    "            self.heads.append(GATLayer(g, input_dim, output_dim))\n",
    "        self.merge = merge\n",
    "    \n",
    "    def forward(self, h) :\n",
    "        head_outs = [head_out(h) for head_out in self.heads]\n",
    "        if self.merge == \"cat\" :\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else :\n",
    "            return torch.mean(torch.stack(head_outs))\n",
    "\n",
    "class GAT(nn.Module) :\n",
    "    def __init__(self, g, input_dim, output_dim, hidden_dim, num_heads) :\n",
    "        super(GAT, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        input_dims = [input_dim] + hidden_dim\n",
    "        output_dims = hidden_dim + [output_dim]\n",
    "\n",
    "        for in_dim, out_dim in zip(input_dims, output_dims) :\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "        \n",
    "        for _ in range(len(hidden_dim)) :\n",
    "            self.activations.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x) :\n",
    "        \n",
    "        for l, activ in zip(self.layers, self.activations) :\n",
    "            x = l(x)\n",
    "            x = activ(x)\n",
    "        return x\n",
    "\n",
    "gnn = GAT(g, input_dim=features.shape[1], hidden_dim=[8], output_dim=7, num_heads=2)\n",
    "print(gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 1,   Loss: 2.0620737075805664\n",
      "Episode: 2,   Loss: 2.0613367557525635\n",
      "Episode: 3,   Loss: 2.060595989227295\n",
      "Episode: 4,   Loss: 2.059852361679077\n",
      "Episode: 5,   Loss: 2.059114694595337\n",
      "Episode: 6,   Loss: 2.058377504348755\n",
      "Episode: 7,   Loss: 2.0576369762420654\n",
      "Episode: 8,   Loss: 2.056898355484009\n",
      "Episode: 9,   Loss: 2.0561611652374268\n",
      "Episode: 10,   Loss: 2.0554234981536865\n",
      "Episode: 11,   Loss: 2.0546860694885254\n",
      "Episode: 12,   Loss: 2.0539486408233643\n",
      "Episode: 13,   Loss: 2.053212881088257\n",
      "Episode: 14,   Loss: 2.0524768829345703\n",
      "Episode: 15,   Loss: 2.051741600036621\n",
      "Episode: 16,   Loss: 2.051006317138672\n",
      "Episode: 17,   Loss: 2.050273895263672\n",
      "Episode: 18,   Loss: 2.0495383739471436\n",
      "Episode: 19,   Loss: 2.0488054752349854\n",
      "Episode: 20,   Loss: 2.048072099685669\n",
      "Episode: 21,   Loss: 2.04733943939209\n",
      "Episode: 22,   Loss: 2.04660701751709\n",
      "Episode: 23,   Loss: 2.0458743572235107\n",
      "Episode: 24,   Loss: 2.045142889022827\n",
      "Episode: 25,   Loss: 2.0444111824035645\n",
      "Episode: 26,   Loss: 2.043679714202881\n",
      "Episode: 27,   Loss: 2.0429489612579346\n",
      "Episode: 28,   Loss: 2.042219400405884\n",
      "Episode: 29,   Loss: 2.041489362716675\n",
      "Episode: 30,   Loss: 2.040759563446045\n",
      "Episode: 31,   Loss: 2.040030002593994\n",
      "Episode: 32,   Loss: 2.0393009185791016\n",
      "Episode: 33,   Loss: 2.0385735034942627\n",
      "Episode: 34,   Loss: 2.0378448963165283\n",
      "Episode: 35,   Loss: 2.0371179580688477\n",
      "Episode: 36,   Loss: 2.036390781402588\n",
      "Episode: 37,   Loss: 2.0356640815734863\n",
      "Episode: 38,   Loss: 2.0349373817443848\n",
      "Episode: 39,   Loss: 2.0342113971710205\n",
      "Episode: 40,   Loss: 2.0334856510162354\n",
      "Episode: 41,   Loss: 2.032761573791504\n",
      "Episode: 42,   Loss: 2.0320353507995605\n",
      "Episode: 43,   Loss: 2.031311511993408\n",
      "Episode: 44,   Loss: 2.0305874347686768\n",
      "Episode: 45,   Loss: 2.029864549636841\n",
      "Episode: 46,   Loss: 2.0291411876678467\n",
      "Episode: 47,   Loss: 2.028418779373169\n",
      "Episode: 48,   Loss: 2.027696132659912\n",
      "Episode: 49,   Loss: 2.02697491645813\n",
      "Episode: 50,   Loss: 2.0262537002563477\n",
      "Episode: 51,   Loss: 2.0255322456359863\n",
      "Episode: 52,   Loss: 2.0248119831085205\n",
      "Episode: 53,   Loss: 2.0240910053253174\n",
      "Episode: 54,   Loss: 2.023371934890747\n",
      "Episode: 55,   Loss: 2.0226519107818604\n",
      "Episode: 56,   Loss: 2.021933078765869\n",
      "Episode: 57,   Loss: 2.021214008331299\n",
      "Episode: 58,   Loss: 2.0204954147338867\n",
      "Episode: 59,   Loss: 2.0197768211364746\n",
      "Episode: 60,   Loss: 2.0190601348876953\n",
      "Episode: 61,   Loss: 2.0183424949645996\n",
      "Episode: 62,   Loss: 2.0176265239715576\n",
      "Episode: 63,   Loss: 2.01690936088562\n",
      "Episode: 64,   Loss: 2.016193389892578\n",
      "Episode: 65,   Loss: 2.0154783725738525\n",
      "Episode: 66,   Loss: 2.0147624015808105\n",
      "Episode: 67,   Loss: 2.0140485763549805\n",
      "Episode: 68,   Loss: 2.013334035873413\n",
      "Episode: 69,   Loss: 2.0126190185546875\n",
      "Episode: 70,   Loss: 2.0119056701660156\n",
      "Episode: 71,   Loss: 2.0111923217773438\n",
      "Episode: 72,   Loss: 2.010478973388672\n",
      "Episode: 73,   Loss: 2.0097670555114746\n",
      "Episode: 74,   Loss: 2.0090548992156982\n",
      "Episode: 75,   Loss: 2.008342981338501\n",
      "Episode: 76,   Loss: 2.007631540298462\n",
      "Episode: 77,   Loss: 2.006920337677002\n",
      "Episode: 78,   Loss: 2.0062098503112793\n",
      "Episode: 79,   Loss: 2.0054984092712402\n",
      "Episode: 80,   Loss: 2.0047898292541504\n",
      "Episode: 81,   Loss: 2.0040810108184814\n",
      "Episode: 82,   Loss: 2.003371238708496\n",
      "Episode: 83,   Loss: 2.0026628971099854\n",
      "Episode: 84,   Loss: 2.0019538402557373\n",
      "Episode: 85,   Loss: 2.0012447834014893\n",
      "Episode: 86,   Loss: 2.0005385875701904\n",
      "Episode: 87,   Loss: 1.9998310804367065\n",
      "Episode: 88,   Loss: 1.9991241693496704\n",
      "Episode: 89,   Loss: 1.9984170198440552\n",
      "Episode: 90,   Loss: 1.997710943222046\n",
      "Episode: 91,   Loss: 1.9970051050186157\n",
      "Episode: 92,   Loss: 1.9962997436523438\n",
      "Episode: 93,   Loss: 1.9955943822860718\n",
      "Episode: 94,   Loss: 1.9948896169662476\n",
      "Episode: 95,   Loss: 1.9941846132278442\n",
      "Episode: 96,   Loss: 1.9934810400009155\n",
      "Episode: 97,   Loss: 1.9927775859832764\n",
      "Episode: 98,   Loss: 1.992073655128479\n",
      "Episode: 99,   Loss: 1.9913700819015503\n",
      "Episode: 100,   Loss: 1.990667462348938\n",
      "Episode: 101,   Loss: 1.9899652004241943\n",
      "Episode: 102,   Loss: 1.989263892173767\n",
      "Episode: 103,   Loss: 1.9885624647140503\n",
      "Episode: 104,   Loss: 1.9878603219985962\n",
      "Episode: 105,   Loss: 1.987160325050354\n",
      "Episode: 106,   Loss: 1.9864588975906372\n",
      "Episode: 107,   Loss: 1.9857587814331055\n",
      "Episode: 108,   Loss: 1.9850600957870483\n",
      "Episode: 109,   Loss: 1.9843592643737793\n",
      "Episode: 110,   Loss: 1.9836599826812744\n",
      "Episode: 111,   Loss: 1.9829614162445068\n",
      "Episode: 112,   Loss: 1.9822629690170288\n",
      "Episode: 113,   Loss: 1.9815645217895508\n",
      "Episode: 114,   Loss: 1.9808673858642578\n",
      "Episode: 115,   Loss: 1.9801703691482544\n",
      "Episode: 116,   Loss: 1.9794732332229614\n",
      "Episode: 117,   Loss: 1.9787760972976685\n",
      "Episode: 118,   Loss: 1.9780796766281128\n",
      "Episode: 119,   Loss: 1.9773849248886108\n",
      "Episode: 120,   Loss: 1.9766887426376343\n",
      "Episode: 121,   Loss: 1.9759944677352905\n",
      "Episode: 122,   Loss: 1.9752992391586304\n",
      "Episode: 123,   Loss: 1.9746052026748657\n",
      "Episode: 124,   Loss: 1.973909854888916\n",
      "Episode: 125,   Loss: 1.9732171297073364\n",
      "Episode: 126,   Loss: 1.9725233316421509\n",
      "Episode: 127,   Loss: 1.9718303680419922\n",
      "Episode: 128,   Loss: 1.971138834953308\n",
      "Episode: 129,   Loss: 1.9704463481903076\n",
      "Episode: 130,   Loss: 1.9697542190551758\n",
      "Episode: 131,   Loss: 1.9690628051757812\n",
      "Episode: 132,   Loss: 1.9683716297149658\n",
      "Episode: 133,   Loss: 1.9676800966262817\n",
      "Episode: 134,   Loss: 1.9669908285140991\n",
      "Episode: 135,   Loss: 1.9663009643554688\n",
      "Episode: 136,   Loss: 1.9656106233596802\n",
      "Episode: 137,   Loss: 1.9649219512939453\n",
      "Episode: 138,   Loss: 1.9642329216003418\n",
      "Episode: 139,   Loss: 1.9635441303253174\n",
      "Episode: 140,   Loss: 1.9628559350967407\n",
      "Episode: 141,   Loss: 1.9621679782867432\n",
      "Episode: 142,   Loss: 1.9614804983139038\n",
      "Episode: 143,   Loss: 1.960793375968933\n",
      "Episode: 144,   Loss: 1.960107684135437\n",
      "Episode: 145,   Loss: 1.9594208002090454\n",
      "Episode: 146,   Loss: 1.958734393119812\n",
      "Episode: 147,   Loss: 1.9580491781234741\n",
      "Episode: 148,   Loss: 1.9573630094528198\n",
      "Episode: 149,   Loss: 1.9566783905029297\n",
      "Episode: 150,   Loss: 1.9559931755065918\n",
      "Episode: 151,   Loss: 1.9553091526031494\n",
      "Episode: 152,   Loss: 1.954625129699707\n",
      "Episode: 153,   Loss: 1.9539415836334229\n",
      "Episode: 154,   Loss: 1.9532577991485596\n",
      "Episode: 155,   Loss: 1.9525761604309082\n",
      "Episode: 156,   Loss: 1.9518934488296509\n",
      "Episode: 157,   Loss: 1.951210856437683\n",
      "Episode: 158,   Loss: 1.9505290985107422\n",
      "Episode: 159,   Loss: 1.9498482942581177\n",
      "Episode: 160,   Loss: 1.949166178703308\n",
      "Episode: 161,   Loss: 1.9484854936599731\n",
      "Episode: 162,   Loss: 1.947806715965271\n",
      "Episode: 163,   Loss: 1.9471256732940674\n",
      "Episode: 164,   Loss: 1.946446418762207\n",
      "Episode: 165,   Loss: 1.945766806602478\n",
      "Episode: 166,   Loss: 1.9450881481170654\n",
      "Episode: 167,   Loss: 1.9444093704223633\n",
      "Episode: 168,   Loss: 1.9437308311462402\n",
      "Episode: 169,   Loss: 1.9430526494979858\n",
      "Episode: 170,   Loss: 1.9423754215240479\n",
      "Episode: 171,   Loss: 1.9416985511779785\n",
      "Episode: 172,   Loss: 1.9410219192504883\n",
      "Episode: 173,   Loss: 1.9403451681137085\n",
      "Episode: 174,   Loss: 1.939669132232666\n",
      "Episode: 175,   Loss: 1.9389927387237549\n",
      "Episode: 176,   Loss: 1.9383184909820557\n",
      "Episode: 177,   Loss: 1.9376428127288818\n",
      "Episode: 178,   Loss: 1.9369678497314453\n",
      "Episode: 179,   Loss: 1.9362937211990356\n",
      "Episode: 180,   Loss: 1.9356200695037842\n",
      "Episode: 181,   Loss: 1.9349452257156372\n",
      "Episode: 182,   Loss: 1.9342719316482544\n",
      "Episode: 183,   Loss: 1.9335989952087402\n",
      "Episode: 184,   Loss: 1.932926893234253\n",
      "Episode: 185,   Loss: 1.9322551488876343\n",
      "Episode: 186,   Loss: 1.9315828084945679\n",
      "Episode: 187,   Loss: 1.9309110641479492\n",
      "Episode: 188,   Loss: 1.930240273475647\n",
      "Episode: 189,   Loss: 1.9295693635940552\n",
      "Episode: 190,   Loss: 1.9288984537124634\n",
      "Episode: 191,   Loss: 1.9282279014587402\n",
      "Episode: 192,   Loss: 1.9275575876235962\n",
      "Episode: 193,   Loss: 1.9268893003463745\n",
      "Episode: 194,   Loss: 1.9262198209762573\n",
      "Episode: 195,   Loss: 1.9255505800247192\n",
      "Episode: 196,   Loss: 1.9248828887939453\n",
      "Episode: 197,   Loss: 1.9242148399353027\n",
      "Episode: 198,   Loss: 1.9235469102859497\n",
      "Episode: 199,   Loss: 1.9228789806365967\n",
      "Episode: 200,   Loss: 1.92221200466156\n",
      "Episode: 201,   Loss: 1.921545386314392\n",
      "Episode: 202,   Loss: 1.9208790063858032\n",
      "Episode: 203,   Loss: 1.9202123880386353\n",
      "Episode: 204,   Loss: 1.9195475578308105\n",
      "Episode: 205,   Loss: 1.9188816547393799\n",
      "Episode: 206,   Loss: 1.918217658996582\n",
      "Episode: 207,   Loss: 1.9175536632537842\n",
      "Episode: 208,   Loss: 1.9168890714645386\n",
      "Episode: 209,   Loss: 1.916225790977478\n",
      "Episode: 210,   Loss: 1.9155609607696533\n",
      "Episode: 211,   Loss: 1.9148980379104614\n",
      "Episode: 212,   Loss: 1.9142355918884277\n",
      "Episode: 213,   Loss: 1.9135724306106567\n",
      "Episode: 214,   Loss: 1.9129106998443604\n",
      "Episode: 215,   Loss: 1.9122495651245117\n",
      "Episode: 216,   Loss: 1.911588191986084\n",
      "Episode: 217,   Loss: 1.9109262228012085\n",
      "Episode: 218,   Loss: 1.9102659225463867\n",
      "Episode: 219,   Loss: 1.909605622291565\n",
      "Episode: 220,   Loss: 1.9089460372924805\n",
      "Episode: 221,   Loss: 1.9082859754562378\n",
      "Episode: 222,   Loss: 1.907626748085022\n",
      "Episode: 223,   Loss: 1.9069679975509644\n",
      "Episode: 224,   Loss: 1.9063084125518799\n",
      "Episode: 225,   Loss: 1.9056507349014282\n",
      "Episode: 226,   Loss: 1.9049935340881348\n",
      "Episode: 227,   Loss: 1.9043363332748413\n",
      "Episode: 228,   Loss: 1.9036784172058105\n",
      "Episode: 229,   Loss: 1.9030216932296753\n",
      "Episode: 230,   Loss: 1.9023653268814087\n",
      "Episode: 231,   Loss: 1.901708722114563\n",
      "Episode: 232,   Loss: 1.9010525941848755\n",
      "Episode: 233,   Loss: 1.9003971815109253\n",
      "Episode: 234,   Loss: 1.899742603302002\n",
      "Episode: 235,   Loss: 1.8990875482559204\n",
      "Episode: 236,   Loss: 1.8984333276748657\n",
      "Episode: 237,   Loss: 1.8977800607681274\n",
      "Episode: 238,   Loss: 1.8971247673034668\n",
      "Episode: 239,   Loss: 1.8964719772338867\n",
      "Episode: 240,   Loss: 1.895818829536438\n",
      "Episode: 241,   Loss: 1.895164966583252\n",
      "Episode: 242,   Loss: 1.8945140838623047\n",
      "Episode: 243,   Loss: 1.893862009048462\n",
      "Episode: 244,   Loss: 1.8932102918624878\n",
      "Episode: 245,   Loss: 1.8925586938858032\n",
      "Episode: 246,   Loss: 1.891908049583435\n",
      "Episode: 247,   Loss: 1.8912566900253296\n",
      "Episode: 248,   Loss: 1.8906071186065674\n",
      "Episode: 249,   Loss: 1.889957308769226\n",
      "Episode: 250,   Loss: 1.889306902885437\n",
      "Episode: 251,   Loss: 1.8886584043502808\n",
      "Episode: 252,   Loss: 1.888008952140808\n",
      "Episode: 253,   Loss: 1.8873600959777832\n",
      "Episode: 254,   Loss: 1.886713981628418\n",
      "Episode: 255,   Loss: 1.8860654830932617\n",
      "Episode: 256,   Loss: 1.8854182958602905\n",
      "Episode: 257,   Loss: 1.8847700357437134\n",
      "Episode: 258,   Loss: 1.884123682975769\n",
      "Episode: 259,   Loss: 1.883476734161377\n",
      "Episode: 260,   Loss: 1.8828299045562744\n",
      "Episode: 261,   Loss: 1.8821849822998047\n",
      "Episode: 262,   Loss: 1.8815394639968872\n",
      "Episode: 263,   Loss: 1.8808943033218384\n",
      "Episode: 264,   Loss: 1.880249261856079\n",
      "Episode: 265,   Loss: 1.8796055316925049\n",
      "Episode: 266,   Loss: 1.8789609670639038\n",
      "Episode: 267,   Loss: 1.8783166408538818\n",
      "Episode: 268,   Loss: 1.8776739835739136\n",
      "Episode: 269,   Loss: 1.8770315647125244\n",
      "Episode: 270,   Loss: 1.8763879537582397\n",
      "Episode: 271,   Loss: 1.8757461309432983\n",
      "Episode: 272,   Loss: 1.8751037120819092\n",
      "Episode: 273,   Loss: 1.8744627237319946\n",
      "Episode: 274,   Loss: 1.8738224506378174\n",
      "Episode: 275,   Loss: 1.8731805086135864\n",
      "Episode: 276,   Loss: 1.8725389242172241\n",
      "Episode: 277,   Loss: 1.8718987703323364\n",
      "Episode: 278,   Loss: 1.8712596893310547\n",
      "Episode: 279,   Loss: 1.8706196546554565\n",
      "Episode: 280,   Loss: 1.8699804544448853\n",
      "Episode: 281,   Loss: 1.8693407773971558\n",
      "Episode: 282,   Loss: 1.8687026500701904\n",
      "Episode: 283,   Loss: 1.8680644035339355\n",
      "Episode: 284,   Loss: 1.867426872253418\n",
      "Episode: 285,   Loss: 1.8667889833450317\n",
      "Episode: 286,   Loss: 1.866152286529541\n",
      "Episode: 287,   Loss: 1.8655147552490234\n",
      "Episode: 288,   Loss: 1.8648779392242432\n",
      "Episode: 289,   Loss: 1.8642425537109375\n",
      "Episode: 290,   Loss: 1.8636069297790527\n",
      "Episode: 291,   Loss: 1.8629719018936157\n",
      "Episode: 292,   Loss: 1.8623360395431519\n",
      "Episode: 293,   Loss: 1.8617013692855835\n",
      "Episode: 294,   Loss: 1.8610671758651733\n",
      "Episode: 295,   Loss: 1.8604331016540527\n",
      "Episode: 296,   Loss: 1.8597990274429321\n",
      "Episode: 297,   Loss: 1.8591653108596802\n",
      "Episode: 298,   Loss: 1.8585320711135864\n",
      "Episode: 299,   Loss: 1.8579001426696777\n",
      "Episode: 300,   Loss: 1.8572664260864258\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "optimizer = optim.Adam(gnn.parameters(), lr=1e-3)\n",
    "\n",
    "for episode in range(300) :\n",
    "    logits = gnn(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[mask], labels[mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Episode: {episode+1},   Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}